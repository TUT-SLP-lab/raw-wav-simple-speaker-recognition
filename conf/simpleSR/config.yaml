# @package _global_

defaults:
  - model: simpleSR
  - optimizer: adamw
    # - scheduler:
  - data: default
  - callbacks: default
  - logger: default
  - criterion: CrossEntropyLoss
  - _self_

seed: 1234
devices: 1  # If you have multi-GPU unit, you can tell here to using GPUs num
ckpt_path:  # If you have pretrained model, you can use this option

train:
  exp_dir: "exp/simpleSR/${now:%Y-%m-%d_%H:%M:%S}"
  out_dir: "${train.exp_dir}/output"
  log_dir: "${train.exp_dir}/log"

  max_epochs: 100
  batch_size: ${data.loader.batch_size}
